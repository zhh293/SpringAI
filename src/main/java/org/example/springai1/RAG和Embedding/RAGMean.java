package org.example.springai1.RAG和Embedding;

public class RAGMean {


 /*
    RAG（Retrieval-Augmented Generation，检索增强生成）


    二、RAG 完整技术流程：5 个核心环节
    RAG 的实现分为 “离线准备阶段” 和 “在线交互阶段”，每个阶段对应不同的技术组件，所有名词都围绕这两个阶段展开。
    阶段 1：离线准备阶段（知识库构建）
    核心目标：将 “非结构化知识”（如 PDF、Word、网页、文档）转化为 “可高效检索的结构化格式”，为后续在线检索做准备。流程拆解：
    文档加载（Document Loading）
    文本清洗与预处理（Text Cleaning & Preprocessing）
    文本分割（Text Chunking）
    文本嵌入（Text Embedding）
    向量存储（Vector Storage）
    阶段 2：在线交互阶段（问答生成）
    核心目标：响应用户实时问题，先检索相关知识，再生成精准回答。流程拆解：
    用户问题预处理（Query Preprocessing）
    相似性检索（Similarity Retrieval）
    结果重排序（Reranking，可选但关键）
    提示构建（Prompt Construction）
    LLM 生成回答（Response Generation）
    三、RAG 关键技术名词解析：定义 + 作用 + 案例
    下面按 “离线准备→在线交互” 的流程，逐一拆解每个技术名词，明确其 “是什么、为什么需要、怎么用”。
            （一）离线准备阶段：知识库构建的核心名词
1. 文档加载（Document Loading）
    定义：将分散的 “非结构化知识源”（如本地 PDF、Word、Excel、网页 HTML、数据库表格、音频转文字文档）统一读取并转化为 “文本格式” 的过程。
    作用：打破知识源的格式壁垒，为后续处理提供统一的文本输入。
    关键工具 / 库：
    开源：LangChain 的PyPDFLoader（加载 PDF）、WebBaseLoader（爬取网页）、Docx2txtLoader（加载 Word）；
    商用：AWS Textract（提取图片 / 扫描件中的文本）、Adobe Acrobat SDK（处理复杂 PDF）。
    常见场景：企业将内部的 “产品手册、客服 FAQ、历史项目文档” 通过加载工具转化为文本。
            2. 文本清洗与预处理（Text Cleaning & Preprocessing）
    定义：对加载后的原始文本进行 “降噪” 和 “标准化” 处理，去除无用信息、统一格式。
    作用：减少干扰信息，提升后续 “分割、嵌入、检索” 的精度（比如乱码、页眉页脚、重复内容会影响语义理解）。
    核心操作：
    降噪：删除页眉页脚、页码、广告、乱码字符；
    标准化：统一大小写（如 “Apple” 和 “apple” 归一）、去除特殊符号（如 “！@#￥”）、纠正错别字；
    结构化：将表格文本（如 Excel 中的行列表格）转化为 “键值对” 或 “自然语句”（如 “2024Q1 营收：1000 万”）。
    示例：将 PDF 中 “第 3 页 产品规格表 2023 版” 这类页眉删除，只保留核心内容 “产品 A 重量：500g，续航：10 小时”。
            3. 文本分割（Text Chunking）：RAG 的 “精度关键”
    定义：将清洗后的 “长文本”（如一篇 10000 字的报告）切割成 “短文本片段”（称为Chunk，通常 50-500 字）的过程。
    为什么必须分割？
            ① LLM 上下文窗口限制：LLM 有最大输入长度（如 GPT-3.5 是 4k tokens，约 3000 字），无法直接处理长文本；
            ② 提升检索精度：若检索时用 “整篇报告” 作为单位，即使问题只关联报告中的某句话，也会返回整篇报告（冗余且低效）；分割后能精准定位 “与问题最相关的小片段”。
    常见分割策略：
    固定长度分割：按字符数 / Token 数切割（如每 200 字一个 Chunk），简单但可能割裂语义（如切割在句子中间）；
    语义感知分割：按 “段落、章节、句子边界” 分割（如用 LangChain 的RecursiveCharacterTextSplitter），优先保证 Chunk 内语义完整；
    领域适配分割：如法律文档按 “法条编号” 分割，代码文档按 “函数 / 类” 分割。
    示例：将一篇 “手机评测报告” 分割为 “外观设计 Chunk”“性能测试 Chunk”“续航测试 Chunk”，用户问 “手机续航多久” 时，只检索 “续航测试 Chunk”。
            4. 文本嵌入（Text Embedding）：RAG 的 “语义桥梁”
    定义：通过 “嵌入模型” 将 “文本 Chunk” 或 “用户问题” 转化为低维稠密向量（称为Embedding Vector，如 768 维、1536 维）的过程。
    核心逻辑：向量的 “距离” 对应文本的 “语义相似度”—— 两个文本语义越近，它们的向量在空间中的距离（如余弦距离）越小；反之越大。
    例：“猫喜欢吃鱼” 和 “猫咪爱吃鱼” 的向量距离很近；“猫喜欢吃鱼” 和 “狗喜欢啃骨头” 的向量距离很远。
    作用：将 “不可计算的文本语义” 转化为 “可计算的向量”，为后续 “相似性检索” 提供基础（计算机无法直接比较文本语义，但能计算向量距离）。
    常见嵌入模型：
    商用：OpenAI text-embedding-3-small（1536 维）、Google text-embedding-004；
    开源：Sentence-BERT（SBERT，轻量高效，适合本地化部署）、BERT-base（通用语义捕捉）、ERNIE（百度，适配中文）。
    注意：嵌入模型需统一 —— 用于处理 “文本 Chunk” 的模型，必须和处理 “用户问题” 的模型一致（否则向量空间不兼容，无法计算相似度）。
            5. 向量存储（Vector Storage）：RAG 的 “知识库容器”
    定义：专门用于存储 “文本 Chunk 对应的 Embedding 向量”，并支持 “高效相似性检索” 的数据库（区别于传统关系型数据库如 MySQL）。
    为什么不用 MySQL？
    传统数据库基于 “关键词匹配”（如 SQL 的LIKE），无法理解语义 —— 比如用户问 “如何解决手机卡顿”，MySQL 无法匹配 “手机运行缓慢的处理方法”（关键词不同但语义相同）；而向量数据库能通过向量距离找到语义相似的 Chunk。
    核心能力：支持近似最近邻搜索（ANN，Approximate Nearest Neighbor） —— 当向量数量达百万 / 千万级时，暴力搜索（遍历所有向量）速度极慢，ANN 能在 “精度” 和 “速度” 间平衡，快速返回 Top-N 个相似向量。
    常见向量数据库：
    商用：Pinecone（云原生，无需运维）、Weaviate（支持多模态，如文本 + 图片向量）、Chroma（轻量，适合开发测试）；
    开源：Milvus（高吞吐，适合大规模数据）、FAISS（Facebook 开源，适合单机部署，轻量快速）、Qdrant（支持地理空间检索）。
    存储结构：向量数据库通常存储 “向量 + 元数据”—— 向量用于检索，元数据包含原始文本 Chunk、文档来源、时间戳等（方便后续生成回答时引用原文）。


            （二）在线交互阶段：问答生成的核心名词
1. 用户问题预处理（Query Preprocessing）
    定义：对用户输入的原始问题（可能模糊、口语化、多意图）进行优化，转化为 “适合检索的清晰查询”。
    作用：减少 “问题模糊导致的检索偏差”—— 比如用户问 “这手机怎么样”，预处理后可补充上下文（如 “用户当前查看的是 iPhone 15，问题应为‘iPhone 15 的性能怎么样’”），提升检索精准度。
    常见操作：
    意图识别：判断用户问题是 “事实查询”（如 “2024 诺奖得主”）还是 “方法咨询”（如 “如何修复电脑蓝屏”）；
    实体提取：提取问题中的关键实体（如 “iPhone 15”“电脑蓝屏”）；
    问题重写：将口语化问题转化为书面语（如 “这手机续航行不”→“该手机的续航能力如何”）。
            2. 相似性检索（Similarity Retrieval）
    定义：将 “预处理后的用户问题” 转化为 Embedding 向量，然后在向量数据库中搜索 “与该向量距离最近的 Top-K 个文本 Chunk”（K 通常取 3-10，即检索 3-10 个最相关的知识片段）。
    核心指标：衡量向量相似度的计算方法，决定检索精度：
    余弦相似度（Cosine Similarity）：最常用，衡量两个向量的 “方向一致性”（取值 - 1~1，越接近 1 语义越相似）；
    欧氏距离（Euclidean Distance）：衡量两个向量的 “空间直线距离”（距离越小越相似，适合低维向量）；
    曼哈顿距离（Manhattan Distance）：衡量向量在各维度上的 “绝对距离和”（抗噪声能力强）。
    作用：从海量知识库中快速筛选出 “与用户问题最相关的知识片段”，为 LLM 提供精准的 “参考资料”。
    示例：用户问 “iPhone 15 续航多久”，检索后返回 3 个 Chunk：①“iPhone 15 标准版续航：视频播放 20 小时”；②“iPhone 15 Pro 续航：视频播放 25 小时”；③“iPhone 15 充电速度：30 分钟充至 50%”。
            3. 结果重排序（Reranking）：提升检索精度的 “优化项”
    定义：对 “相似性检索返回的 Top-K 个 Chunk” 进行二次排序，调整其优先级，让 “最相关的 Chunk” 排在最前面。
    为什么需要重排序？
    初检索（基于 ANN）为了速度，可能存在 “语义偏差”—— 比如检索时优先匹配 “关键词” 而非 “深层语义”，导致部分相关度低的 Chunk 排在前面。重排序通过更精细的语义模型，修正这种偏差。
    核心技术：
    模型：用 “交叉编码器（Cross-Encoder）” 替代初检索的 “双编码器（Bi-Encoder）”—— 双编码器只单独计算 “问题向量” 和 “Chunk 向量” 的距离，交叉编码器会同时输入 “问题 + Chunk”，计算两者的 “交互语义相似度”，精度更高（但速度稍慢）；
    工具：开源的Cross-Encoder库（如 sentence-transformers/cross-encoder）、商用的 Cohere Rerank API。
    作用：将检索结果的 “相关性准确率” 提升 10%-30%，确保 LLM 优先使用最有用的知识。
            4. 提示构建（Prompt Construction）：引导 LLM 的 “说明书”
    定义：将 “用户问题 + 重排序后的 Top-K 个 Chunk + 指令” 组合成一段 “结构化文本”（称为Prompt），作为 LLM 的输入。
    核心逻辑：LLM 的生成质量高度依赖 Prompt—— 若只给问题，LLM 可能凭记忆回答（易幻觉）；若明确 “基于以下参考文档回答”，LLM 会优先使用检索到的知识，减少幻觉。
    常见 Prompt 模板（以中文为例）：
    plaintext
    请基于以下参考文档，回答用户问题。要求：
            1. 仅使用参考文档中的信息，不添加外部知识；
            2. 若参考文档中没有相关信息，直接回答“无法找到相关答案”；
            3. 回答需简洁明了，分点说明（若有多个要点）。

            【参考文档】
            1. Chunk1内容：iPhone 15标准版续航：视频播放20小时，音频播放80小时，支持20W有线充电。
            2. Chunk2内容：iPhone 15 Pro续航：视频播放25小时，音频播放95小时，支持30W有线充电。

            【用户问题】
    iPhone 15标准版和Pro版的续航有什么区别？
    作用：约束 LLM 的生成逻辑，确保回答 “基于知识、无幻觉、符合格式要求”。
            5. LLM 生成回答（Response Generation）
    定义：将构建好的 Prompt 输入给 LLM，让 LLM 基于 “参考文档” 生成最终的自然语言回答。
    核心要求：LLM 需具备 “知识整合能力”—— 能从多个 Chunk 中提取关键信息，合并成连贯的回答（而非简单拼接 Chunk 内容）。
    常见 LLM 选择：
    商用：GPT-4o、Claude 3 Opus（长上下文支持好）、文心一言 4.0、通义千问 X；
    开源：Llama 3（70B 参数版，本地化部署）、Qwen-72B（阿里，中文支持好）、Mistral 8x7B（高效平衡）。
    作用：将 “碎片化的知识 Chunk” 转化为 “用户易懂的自然语言回答”，完成最终的交互闭环。*/










}
